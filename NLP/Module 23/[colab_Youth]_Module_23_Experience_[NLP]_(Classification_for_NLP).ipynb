{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "[colab_Youth]_Module_23_Experience_[NLP]_(Classification_for_NLP).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8aSLWCQjZNfv",
        "FiiWUoqCYzs_",
        "K92k246vYztA",
        "_tT3llt2Yzvi",
        "p19UBH5hYzzg"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aSLWCQjZNfv"
      },
      "source": [
        "# NLP Lesson Map: NLP Process\n",
        "\n",
        "<font color=red>Red </font> highlighted is the topic covered for this lesson\n",
        "\n",
        "1.\t<font color=red>Obtain Text</font> \n",
        "> *\tFrom NTLK \n",
        "> *\tFrom website\n",
        "> *\t<font color=red> From CSV</font>\n",
        "2.\t<font color=red>Tokens\n",
        "> *\tSentence Segmentation\n",
        "> *\tTokenization\n",
        "> *\tRemove stopwords, special characters, numbers\n",
        "> *\tConverting text to a common case</font>\n",
        "> *\tStemming & Lemmatization\n",
        "3.\t<font color=red>Numbers\n",
        "> *\tCreate a Dictionary\n",
        "> *\tCreate Document Vectors = Bag of Words = Term Frequency (tf)  = number of times that each word occurs per document\n",
        "> *\tidf\n",
        "> *\ttf-idf</font>\n",
        "4.\t<font color=red>AI Models\n",
        "> * Logistic Regression</font>\n",
        "> *\tCosine Similarity\n",
        "> *\tNeural Network\n",
        "5.\t<font color=red>NLP Applications\n",
        "> * Classification\n",
        "> *\tSentiment Analysis</font>\n",
        "> *\tChatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3cepmydYzsY"
      },
      "source": [
        "# Classification of text data\n",
        "\n",
        "One popular application of Natural Language processing is to classify text data. Classification can be done in various form: we can classify if a tweet is related to a particular topic or not, or we can classify if a particular review leans towards positive or negative. \n",
        "\n",
        "Today, we will hone our skills in classifying NLP data using NLP! \n",
        "Firstly, we will collect, analyze and process our data. We will be using bag of words and tf-idf (remember the activity that we have done in the acquire stage?). These processes turn texts into numbers. \n",
        "We will then take the word vectors we have created and use a machine learning algorithm to learn from the data and come up with a model to do our classification task. \n",
        "\n",
        "Let's start!\n",
        "\n",
        "For our first task, we will use a collection of tweet data to predict if the tweets are referring to natural disasters, or just regular tweets.\n",
        "\n",
        "Let's first import the required libraries!\n",
        "\n",
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jpt6t-oYzsb"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH5RJe9gYzsi"
      },
      "source": [
        "## Open csv file\n",
        "Do you have the tweet data file with you? If not, refer to Experience module 1 to see how you can download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItaXkHH4Y2_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c514c92-0d08-46ca-ae74-4f14f043b998"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/Intel_AI4Y')"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/Intel_AI4Y; to attempt to forcibly remount, call drive.mount(\"/content/Intel_AI4Y\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSLFCZk3Yzsj"
      },
      "source": [
        "df = pd.read_csv('/content/Intel_AI4Y/My Drive/Intel_AI4Y/Students_E_Learning/Copy_To_Google_Drive/Intel_AI4Y_Colab/Module_23/tweet/disasters_social_media.csv', encoding='latin-1')"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF3Vn_GkYzs0"
      },
      "source": [
        "### Analyzing our data for classification\n",
        "Before we begin working with the data, let us first look at some of the data's features. You should have an idea of the data structure from the data analysis in Experience 1.\n",
        "\n",
        "<font color=red>Your code:</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef4SCgz0Yzs1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "outputId": "2eaafff2-4f90-4e08-bf84-fe319dbae3cc"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_unit_id</th>\n",
              "      <th>_golden</th>\n",
              "      <th>_unit_state</th>\n",
              "      <th>_trusted_judgments</th>\n",
              "      <th>_last_judgment_at</th>\n",
              "      <th>choose_one</th>\n",
              "      <th>choose_one:confidence</th>\n",
              "      <th>choose_one_gold</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>tweetid</th>\n",
              "      <th>userid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>778243823</td>\n",
              "      <td>True</td>\n",
              "      <td>golden</td>\n",
              "      <td>156</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>778243824</td>\n",
              "      <td>True</td>\n",
              "      <td>golden</td>\n",
              "      <td>152</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>778243825</td>\n",
              "      <td>True</td>\n",
              "      <td>golden</td>\n",
              "      <td>137</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>778243826</td>\n",
              "      <td>True</td>\n",
              "      <td>golden</td>\n",
              "      <td>136</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>0.9603</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>778243827</td>\n",
              "      <td>True</td>\n",
              "      <td>golden</td>\n",
              "      <td>138</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>16.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    _unit_id  _golden  ... tweetid  userid\n",
              "0  778243823     True  ...     1.0     NaN\n",
              "1  778243824     True  ...    13.0     NaN\n",
              "2  778243825     True  ...    14.0     NaN\n",
              "3  778243826     True  ...    15.0     NaN\n",
              "4  778243827     True  ...    16.0     NaN\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIW_Y9E0Yzs7"
      },
      "source": [
        "The `head()` function allows us to see the first few rows from the dataset. Do you wonder how many rows do we have here?\n",
        "\n",
        "### Task: print out the length of the dataframe\n",
        "\n",
        "<font color=red>Your code:</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AcjZKzdYzs8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e173c0d1-2dae-4535-9815-e033724e80b3"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10876"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiiWUoqCYzs_"
      },
      "source": [
        "What headings do you see above? Which headings do you think are important?\n",
        "\n",
        "### What is a 'target'?\n",
        "We call the field we are trying to predict the 'target'. In this case, the target is whether the tweet is relevant to a natural disaster or irrelevant. These values are reflected in the `['choose_one']` column (See it above NOW!). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K92k246vYztA"
      },
      "source": [
        "### Remember the 'labels'?\n",
        "In this dataset, the labels of the target has been filled out by human volunteers. When you are working on your own datasets in the future, you may have to label them manually, or find volunteers to do so.\n",
        "\n",
        "This is usually an expensive task to do in terms of effort and time. There are even [online platforms](https://www.mturk.com/) where you can find workers for this job!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29EgF2FrYztC"
      },
      "source": [
        "### Check labels\n",
        "\n",
        "Let's look at the the categories that the tweets have been classified into. To do that, we can look for the number of unique values within that column. The python's built-in function `set()` takes in a list of values and outputs the total unique values. Let us see how it works! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCBku5eLYztD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b03632c8-759b-4021-c156-d06cce496e56"
      },
      "source": [
        "set(['apple', 'orange', 'apple', 'orange', 'pears'])"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'apple', 'orange', 'pears'}"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f2dLoFHYztH"
      },
      "source": [
        "Can you make sense of the code above? You have only 3 unique values in a list of 5, and only the unit values are printed out. \n",
        "\n",
        "### Task: Change the function such that you have 2 unique values and 6 values in the list!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JGxKGr3YztH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cfab8ec-df67-40be-e749-b2aa1e0e82f5"
      },
      "source": [
        "set(['apple', 'orange', 'apple', 'orange', 'orange','apple'])"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'apple', 'orange'}"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW6I2X7NYztK"
      },
      "source": [
        "The piece of code below list down the values of the column 'choose_one', which is a measure of relevance of a particular tweet to natural disaster. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PioWbIoFYztL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea1b8741-4dbe-4006-aa59-b9a332c022d1"
      },
      "source": [
        "df.choose_one.values"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Relevant', 'Relevant', 'Relevant', ..., 'Relevant', 'Relevant',\n",
              "       'Relevant'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZu30JzXYztQ"
      },
      "source": [
        "Without using the set() funciton, can you guess how many potential values might this column have? \n",
        "\n",
        "### Task: find out the number of unique relevance values on the 'choose_one' column using the set() function.\n",
        "\n",
        "<font color=red>Your code:</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgR9DpvbYztR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "393c91b6-60be-4a96-a51c-438ca92f50a0"
      },
      "source": [
        "set(df.choose_one)"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"Can't Decide\", 'Not Relevant', 'Relevant'}"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbW1oIz2YztV"
      },
      "source": [
        "Makes sense? A tweet can either be related to a natural disaster, not related to a natural disaster, and there are cases when the people who labelled the data cannot decide if the tweet is related to natural disaster or not. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Urz-fAFYztW"
      },
      "source": [
        "For now, we only care about predicting in a binary fashion (relevant vs not relevant), so we discard the 'Can't decide' class. Remember how we [subset data using criteria](http://chris.friedline.net/2015-12-15-rutgers/lessons/python2/02-index-slice-subset.html) on pandas dataframe?  \n",
        "\n",
        "### Task: Subset the dataframe and only take rows that does not have 'Can't Decide' in the choose_one column\n",
        "\n",
        "<font color=red>Hints:</font> \n",
        "*   Refer to Module 16 Assignment 3 \"Handling_erroneous_and_missing_data\" at \"The code should be able to remove rows with Mathematics score that are lesser than 0 or more than 100. Try out the code below!\"\n",
        "*   Or refer to Module 16 Assignment 2 \"Basic_data_processing_and_visualisation\" last Bonus question: similar to \"`df_Setosa = df[df['class'] == 'Iris-setosa']`\"\n",
        "*   Check out 'not equal' operator: https://www.tutorialspoint.com/python/python_basic_operators.htm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRg-XFbIYztX"
      },
      "source": [
        "#your code \n",
        "to_drop = df[df[\"choose_one\"] == \"Can't Decide\"]\n",
        "df.drop(to_drop.index, inplace=True)"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNLY2cp8YztZ"
      },
      "source": [
        "Print out your dataframe and see what you have done\n",
        "\n",
        "<font color=red>Your code:</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xybz0jWyYzta",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "outputId": "06a93dfc-c644-4640-89f8-06ab9db98ef7"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_unit_id</th>\n",
              "      <th>_golden</th>\n",
              "      <th>_unit_state</th>\n",
              "      <th>_trusted_judgments</th>\n",
              "      <th>_last_judgment_at</th>\n",
              "      <th>choose_one</th>\n",
              "      <th>choose_one:confidence</th>\n",
              "      <th>choose_one_gold</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>tweetid</th>\n",
              "      <th>userid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>778243823</td>\n",
              "      <td>True</td>\n",
              "      <td>golden</td>\n",
              "      <td>156</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>778243824</td>\n",
              "      <td>True</td>\n",
              "      <td>golden</td>\n",
              "      <td>152</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>778243825</td>\n",
              "      <td>True</td>\n",
              "      <td>golden</td>\n",
              "      <td>137</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>778243826</td>\n",
              "      <td>True</td>\n",
              "      <td>golden</td>\n",
              "      <td>136</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>0.9603</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>778243827</td>\n",
              "      <td>True</td>\n",
              "      <td>golden</td>\n",
              "      <td>138</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>16.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    _unit_id  _golden  ... tweetid  userid\n",
              "0  778243823     True  ...     1.0     NaN\n",
              "1  778243824     True  ...    13.0     NaN\n",
              "2  778243825     True  ...    14.0     NaN\n",
              "3  778243826     True  ...    15.0     NaN\n",
              "4  778243827     True  ...    16.0     NaN\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNOVwKGxYzte"
      },
      "source": [
        "Check out the length of your dataframe now. Does it decreases or stay the same?\n",
        "\n",
        "### Print out the length of the dataframe\n",
        "<font color=red>Your code:</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeV5T3v5Yztg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee42e81-4e5f-43dc-9fa0-bad7e7a82984"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10860"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN1Mvwu1Yztm"
      },
      "source": [
        "Now, we want to focus on only columns 'text' and 'choose_one'\n",
        "\n",
        "### Task: Subset the dataframe to only take the columns 'text' and 'choose_one'\n",
        "See Selecting Data Using Labels (Column Headings) from this [article](http://chris.friedline.net/2015-12-15-rutgers/lessons/python2/02-index-slice-subset.html)!\n",
        "\n",
        "<font color=red>Your code:</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqOFrNPGYztn"
      },
      "source": [
        "\n",
        "del df['_golden']\n",
        "del df['_unit_state']\n",
        "del df['_trusted_judgments']\n",
        "del df['_last_judgment_at']\n",
        "del df['choose_one:confidence']\n",
        "del df['choose_one_gold']\n",
        "del df['keyword']\n",
        "del df['location']\n",
        "del df['tweetid']\n",
        "del df['userid']\n"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hLudkMM3lkC"
      },
      "source": [
        "del df['_unit_id']"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BkU9i0UYztq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "4e785018-c46f-4353-b395-0d0bda6d8c80"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>choose_one</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Relevant</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Relevant</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Relevant</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Relevant</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Relevant</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  choose_one                                               text\n",
              "0   Relevant                 Just happened a terrible car crash\n",
              "1   Relevant  Our Deeds are the Reason of this #earthquake M...\n",
              "2   Relevant  Heard about #earthquake is different cities, s...\n",
              "3   Relevant  there is a forest fire at spot pond, geese are...\n",
              "4   Relevant             Forest fire near La Ronge Sask. Canada"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CksRN41SYztt"
      },
      "source": [
        "We also [map](https://chrisalbon.com/python/data_wrangling/pandas_map_values_to_values/) these values on to numbers 1 for relevant tweets, and 0 for not relevant tweets. \n",
        "\n",
        "### Task: Map 'Relevant' into 1 and 'Not Relevant' into 0 and put it into a new column called 'relevance'\n",
        "\n",
        "<font color=red>Hints:</font>\n",
        "*   Please do not miss the '`#your code here`'\n",
        "*   Read about `.map()` at the given link: https://chrisalbon.com/python/data_wrangling/pandas_map_values_to_values/\n",
        "*   Dictionary creation: https://www.w3schools.com/python/python_dictionaries.asp or https://www.tutorialspoint.com/python/python_dictionary.htm \n",
        "*   Map to `1` and `0` => <font color=red>NOT '1' and '0'</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwJgq00mYztv"
      },
      "source": [
        "relevance = {'Relevant':1, 'Not Relevant':0}\n",
        "df['relevance'] = df.choose_one.map(relevance) # map relevant values to the number 1, and not relevant to the value 0"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZFv3p5xYzty",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "4567672e-24d9-4b83-e5fa-81ba7f8b1673"
      },
      "source": [
        "df"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>choose_one</th>\n",
              "      <th>text</th>\n",
              "      <th>relevance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Relevant</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Relevant</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Relevant</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Relevant</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Relevant</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10871</th>\n",
              "      <td>Relevant</td>\n",
              "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10872</th>\n",
              "      <td>Relevant</td>\n",
              "      <td>Police investigating after an e-bike collided ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10873</th>\n",
              "      <td>Relevant</td>\n",
              "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10874</th>\n",
              "      <td>Relevant</td>\n",
              "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10875</th>\n",
              "      <td>Relevant</td>\n",
              "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10860 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      choose_one                                               text  relevance\n",
              "0       Relevant                 Just happened a terrible car crash          1\n",
              "1       Relevant  Our Deeds are the Reason of this #earthquake M...          1\n",
              "2       Relevant  Heard about #earthquake is different cities, s...          1\n",
              "3       Relevant  there is a forest fire at spot pond, geese are...          1\n",
              "4       Relevant             Forest fire near La Ronge Sask. Canada          1\n",
              "...          ...                                                ...        ...\n",
              "10871   Relevant  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...          1\n",
              "10872   Relevant  Police investigating after an e-bike collided ...          1\n",
              "10873   Relevant  The Latest: More Homes Razed by Northern Calif...          1\n",
              "10874   Relevant  MEG issues Hazardous Weather Outlook (HWO) htt...          1\n",
              "10875   Relevant  #CityofCalgary has activated its Municipal Eme...          1\n",
              "\n",
              "[10860 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2y3YvHR4D6u"
      },
      "source": [
        "columns_titles = [\"text\", \"choose_one\", 'relevance']\n",
        "df = df.reindex(columns=columns_titles)"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "j7zpzrHx4ONt",
        "outputId": "c475d3fc-ba9a-4c40-fa05-18396e0d1358"
      },
      "source": [
        "df"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>choose_one</th>\n",
              "      <th>relevance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10871</th>\n",
              "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10872</th>\n",
              "      <td>Police investigating after an e-bike collided ...</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10873</th>\n",
              "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10874</th>\n",
              "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10875</th>\n",
              "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10860 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text choose_one  relevance\n",
              "0                     Just happened a terrible car crash   Relevant          1\n",
              "1      Our Deeds are the Reason of this #earthquake M...   Relevant          1\n",
              "2      Heard about #earthquake is different cities, s...   Relevant          1\n",
              "3      there is a forest fire at spot pond, geese are...   Relevant          1\n",
              "4                 Forest fire near La Ronge Sask. Canada   Relevant          1\n",
              "...                                                  ...        ...        ...\n",
              "10871  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   Relevant          1\n",
              "10872  Police investigating after an e-bike collided ...   Relevant          1\n",
              "10873  The Latest: More Homes Razed by Northern Calif...   Relevant          1\n",
              "10874  MEG issues Hazardous Weather Outlook (HWO) htt...   Relevant          1\n",
              "10875  #CityofCalgary has activated its Municipal Eme...   Relevant          1\n",
              "\n",
              "[10860 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAW_MRr5Yzt5"
      },
      "source": [
        "Look at what we have done!\n",
        "- We've chosen only the column we are interested in, reducing the columns from 13 to just 3!\n",
        "- We've mapped 'Relevant' to 1 and 'Not Relevant' to 0\n",
        "\n",
        "Now, we will proceed with text processing, followed with bag of words and tf-idf!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRMCXFh4Yzt7"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "The first step is to write functions to normalize and tokenize the tweets (We covered this in Experience 1). An example is given below, but you can and should improve it by utilizing the skills you have learnt previously. For example, by adding additional ignore-words, or by lemming or stemming the dataset. The more you pre-process the data, the better your model can be!  \n",
        "  \n",
        "Why do you think we choose to ignore those words in the ignore list?\n",
        "\n",
        "<font color=red>Hints:</font> \n",
        "\n",
        "\n",
        "*   Don't missed out `#your code` here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUUrM8hxYzt8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4575932-0af6-4915-e4a1-ee110c530622"
      },
      "source": [
        "def extract_words(sentence):\n",
        "    '''This is to clean and tokenize words'''\n",
        "    ignore_words = ['a', 'the', 'if', 'br', 'and', 'of', 'to', 'is']\n",
        "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() # this replaces all special chars with ' '\n",
        "    words_cleaned = [w.lower() for w in words if w not in ignore_words]\n",
        "    return words_cleaned \n",
        "\n",
        "# let us test this out!\n",
        "test_sentence = 'Good morning, how are you today? It is a good day.'\n",
        "print(extract_words(test_sentence))"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['good', 'morning', 'how', 'are', 'you', 'today', 'it', 'good', 'day']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aikyQvXeYzt_"
      },
      "source": [
        "### Task: Add at least 5 more stop words into the `extract_words` function\n",
        "\n",
        "<font color=red>Your code:</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDSQ2LPwYzuB"
      },
      "source": [
        "def extract_words(sentence):\n",
        "    '''This is to clean and tokenize words'''\n",
        "    ignore_words = ['a', 'the', 'if', 'br', 'and', 'of', 'to', 'is', 'it', 'may', 'you']\n",
        "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() # this replaces all special chars with ' '\n",
        "    words_cleaned = [w.lower() for w in words if w not in ignore_words]\n",
        "    return words_cleaned \n",
        "\n"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q48jgNKLYzuG"
      },
      "source": [
        "### Task: Test your new stop words on the same sentence!\n",
        "\n",
        "<font color=red>Your code:</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luqjJUKFYzuH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "860989e9-4667-45f8-8dcf-90f0e14101c9"
      },
      "source": [
        "# let us test this out!\n",
        "test_sentence = 'Good morning, how are you today? It is a good day.'\n",
        "print(extract_words(test_sentence))"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['good', 'morning', 'how', 'are', 'today', 'it', 'good', 'day']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D_EM5cxYzuh"
      },
      "source": [
        "do you notice that the word 'it' is still present? do you know why?\n",
        "\n",
        "### Task: add a function to lower the case in `extract_words` function\n",
        "See [here](https://machinelearningmastery.com/clean-text-machine-learning-python/) on how to do it. \n",
        "\n",
        "<font color=red>Your code:</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDJ-JRTcYzui"
      },
      "source": [
        "def extract_words(sentence):\n",
        "    '''This is to clean and tokenize words'''\n",
        "    ignore_words = ['a', 'the', 'if', 'br', 'and', 'of', 'to', 'is', 'are', 'he', 'she', 'my', 'you', 'it','how']\n",
        "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() # this replaces all special chars with ' '\n",
        "    words = [word.lower() for word in words]\n",
        "    words_cleaned = [w.lower() for w in words if w not in ignore_words]\n",
        "    return words_cleaned "
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I8W1hGYYzur"
      },
      "source": [
        "Try again with the same sentence!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXjco1HoYzus",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecc7e0b3-88d6-4513-ab78-51e81dfe3e85"
      },
      "source": [
        "# let us test this out!\n",
        "test_sentence = 'Good morning, how are you today? It is a good day.'\n",
        "print(extract_words(test_sentence))"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['good', 'morning', 'today', 'good', 'day']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln5bVafKYzuy"
      },
      "source": [
        "Great! Feel free to add your own processing into the pipeline e.g. stemming or lemmatization. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKbjerYZYzuz"
      },
      "source": [
        "## 1. Bag of words\n",
        "Now that we have a function to pre-process our textual data, we can proceed with converting our textual data into numbers. The simplest way to do this is using the bag of words algorithm. \n",
        "\n",
        "In a bag of words, we count the number of times each word appears for each tweet and use those counts as our input data. This is accomplished in the following steps:\n",
        "1. Create a vocabulary of all words that appear in your corpus (a corpus is a collection of all your text data, i.e. all tweets)\n",
        "2. Turn that vocabulary into a vector. i.e. if there are 500 unique words in your corpus, the vector will have a length of 500, with each position corresponding to a word in the corpus.\n",
        "3. For each document (tweet), count the number of times every word appears and add those numbers into the vector. This will result in each document having its own vector of length 500, that represents all the words appearing in the document.\n",
        "\n",
        "### Example \n",
        "consider a corpus consisting of two documents: \n",
        "1. 'I love NLP', \n",
        "2. 'I love machine learning'. \n",
        "\n",
        "#### Vocbulary\n",
        "The vocabulary will be a vector of length 5, consisting of the words: \n",
        "\n",
        "'I', 'love', 'NLP', 'machine', 'learning'.  \n",
        "\n",
        "#### Vector\n",
        "The vector for the first sentence (number 1) will be: \n",
        "[1, 1, 1, 0, 0] since it contains 'I', 'love', 'NLP', but not 'machine', and 'learning'.  \n",
        "\n",
        "Can you construct the vector for 2?  \n",
        "\n",
        "Combining vectors for 1 and 2, the bag of words will be an array with vector 1 in the first row, and vector 2 in the second row. \n",
        "\n",
        "Now let us implement this algorithm!  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU3lANVZYzu1"
      },
      "source": [
        "### Building the bag of words\n",
        "\n",
        "First, we would like to know how often each individual word appears in our dataset. \n",
        "\n",
        "We can represent this in the form of a dictionary, which has the format {'word':frequency}, where each key is a word, and the frequencies are the number of times the words appears in our dataset. \n",
        "\n",
        "If you would like to learn more about dictionaries, visit [python dictionaries](https://www.w3schools.com/python/python_dictionaries.asp).  \n",
        "\n",
        "### Hash map\n",
        "\n",
        "This dictionary is known as a hash map, and it can be built progressively by looping through each token in the document. \n",
        "\n",
        "If the token can not be found in the hash map, add the token to the hash map, and set it's frequency as 1. If the token already exists, increment the frequency by 1.  \n",
        "  \n",
        "We will do this in two functions. \n",
        "1. First, build a function called map_book that takes in a dictionary called the hash_map, as well as the tokens from a tweet, and updates the hash_map with each word in the tokens. \n",
        "2. Next, build a function (you can call it make_hash_map) that can loop through all the tweets, and calls the first function to update the hash_map.  \n",
        "  \n",
        "<font color=red>*Hint:</font> You can loop through your tokens by using `for word in tokens:`.  \n",
        "You can check if the word exists in your hash_map by using `if word in hash_map:`  \n",
        "you can assess the counts of your hash map by using `hash_map[word]`. Increment this by 1 to increase the count."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnT7vIegYzu5"
      },
      "source": [
        "This is the function map_book. Can you make sense of it?\n",
        "\n",
        "<font color=red>Hints:</font>\n",
        "*   Please do not miss out the '`#your code here`'\n",
        "*   Remember varaible increment is like this `i = i + 1`\n",
        "*   If `map` is not in `hasp_map`, please set `hash_map[word]` as `1`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47JztnM9Yzu6"
      },
      "source": [
        "# calculate frequency of words\n",
        "def map_book(hash_map, tokens):\n",
        "    if tokens is not None:\n",
        "        for word in tokens:\n",
        "            # Word Exist?\n",
        "            if word in hash_map:\n",
        "                hash_map[word] = hash_map[word] + 1\n",
        "            else:\n",
        "                hash_map[word] = 1\n",
        "\n",
        "        return hash_map\n",
        "    else:\n",
        "        return None"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGxEIGpfYzvC"
      },
      "source": [
        "This is the function `make_hash_map`. Can you make sense of it?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5MujridYzvD"
      },
      "source": [
        "def make_hash_map(df):\n",
        "    hash_map = {}\n",
        "    for index, row in df.iterrows():\n",
        "        hash_map = map_book(hash_map, extract_words(row['text']))\n",
        "    return hash_map"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr8xJBYpYzvL"
      },
      "source": [
        "### Redefining our dictionary\n",
        "\n",
        "While you can construct your bag of words using all words in all tweets, it can become too much for your computer very quickly. A good solution is to take just a few hundred or thousand of the most common words. We will redefine our dictionary to consist of just the 500 most popular tokens.  \n",
        "  \n",
        "How can we do this? Remember that we have just constructed a hash map which is a dictionary with each token as a key, and the number of times the token has appeared as the value. Build a function called `frequent_vocab` that takes in the hash_map and the maximum vocabulary, and returns a list of the most popular tokens as defined by the maximum vocabulary (set this to 500 for now)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONFpsk9rYzvN"
      },
      "source": [
        "This is the function `frequent_vocab`. Can you make sense of it?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih2l6206YzvP"
      },
      "source": [
        "# define a function frequent_vocab with the following input: word_freq and max_features\n",
        "def frequent_vocab(word_freq, max_features): \n",
        "    counter = 0  #initialize counter with the value zero\n",
        "    vocab = []   # create an empty list called vocab\n",
        "    # list words in the dictionary in descending order of frequency\n",
        "    for key, value in sorted(word_freq.items(), key=lambda item: (item[1], item[0]), reverse=True): \n",
        "       #loop function to get the top (max_features) number of words\n",
        "        if counter<max_features: \n",
        "            vocab.append(key)\n",
        "            counter+=1\n",
        "        else: break\n",
        "    return vocab"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfWMYxhcYzvb"
      },
      "source": [
        "### Experiment! What happens if you change the above function to (`reverse = False`)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTBL8Pw-Yzvc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30a49d65-979d-424e-cc48-069e55923f77"
      },
      "source": [
        "hash_map = make_hash_map(df) #create hash map (words and frequency) from tokenized dataset\n",
        "\n",
        "vocab=frequent_vocab(hash_map, 500)\n",
        "vocab"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['t',\n",
              " 'co',\n",
              " 'http',\n",
              " 'in',\n",
              " 'i',\n",
              " 's',\n",
              " 'for',\n",
              " 'on',\n",
              " 'that',\n",
              " 'with',\n",
              " 'by',\n",
              " 'at',\n",
              " 'this',\n",
              " 'https',\n",
              " 'from',\n",
              " 'be',\n",
              " 'was',\n",
              " 'û_',\n",
              " 'have',\n",
              " 'amp',\n",
              " 'like',\n",
              " 'as',\n",
              " 'up',\n",
              " 'just',\n",
              " 'me',\n",
              " 'we',\n",
              " 'but',\n",
              " 'so',\n",
              " 'not',\n",
              " 'your',\n",
              " 'm',\n",
              " 'out',\n",
              " 'no',\n",
              " 'all',\n",
              " 'will',\n",
              " 'after',\n",
              " 'fire',\n",
              " 'can',\n",
              " 'an',\n",
              " 'when',\n",
              " 'has',\n",
              " 'get',\n",
              " 'new',\n",
              " 'via',\n",
              " 'they',\n",
              " 'more',\n",
              " 'about',\n",
              " 'what',\n",
              " '2',\n",
              " 'now',\n",
              " 'or',\n",
              " 'news',\n",
              " 'people',\n",
              " 'one',\n",
              " 'who',\n",
              " 'there',\n",
              " 'over',\n",
              " 'don',\n",
              " 'been',\n",
              " 'do',\n",
              " 'into',\n",
              " 're',\n",
              " 'emergency',\n",
              " 'video',\n",
              " 'disaster',\n",
              " 'would',\n",
              " '3',\n",
              " 'police',\n",
              " 'her',\n",
              " 'his',\n",
              " 'u',\n",
              " 'than',\n",
              " 'were',\n",
              " 'still',\n",
              " 'some',\n",
              " 'body',\n",
              " 'suicide',\n",
              " 'us',\n",
              " '1',\n",
              " 'why',\n",
              " 'storm',\n",
              " 'off',\n",
              " 'first',\n",
              " 'time',\n",
              " 'burning',\n",
              " 'crash',\n",
              " 'them',\n",
              " 'rt',\n",
              " 'attack',\n",
              " 'had',\n",
              " 'got',\n",
              " 'back',\n",
              " 'california',\n",
              " 'day',\n",
              " 'know',\n",
              " 'fires',\n",
              " 'two',\n",
              " 'man',\n",
              " 'our',\n",
              " 'buildings',\n",
              " 'going',\n",
              " 'today',\n",
              " 'see',\n",
              " 'world',\n",
              " 'nuclear',\n",
              " 'bomb',\n",
              " 'love',\n",
              " 'hiroshima',\n",
              " 'year',\n",
              " 'full',\n",
              " 'go',\n",
              " '5',\n",
              " 'û',\n",
              " 'dead',\n",
              " 'youtube',\n",
              " 'life',\n",
              " 'watch',\n",
              " 'old',\n",
              " 'car',\n",
              " 'their',\n",
              " 'killed',\n",
              " 'train',\n",
              " 'think',\n",
              " '4',\n",
              " 'only',\n",
              " 'last',\n",
              " 'ûªs',\n",
              " 'here',\n",
              " 'down',\n",
              " 'accident',\n",
              " 'let',\n",
              " 'war',\n",
              " 'good',\n",
              " 'make',\n",
              " 'being',\n",
              " 'say',\n",
              " 'gt',\n",
              " '2015',\n",
              " 'w',\n",
              " 'could',\n",
              " 'way',\n",
              " 'may',\n",
              " 'many',\n",
              " 'families',\n",
              " 'need',\n",
              " 'even',\n",
              " 'years',\n",
              " 'want',\n",
              " 'best',\n",
              " 'because',\n",
              " 've',\n",
              " 'then',\n",
              " 'mass',\n",
              " 'll',\n",
              " 'did',\n",
              " 'bombing',\n",
              " 'too',\n",
              " 'collapse',\n",
              " 'right',\n",
              " 'home',\n",
              " 'him',\n",
              " 'water',\n",
              " 'forest',\n",
              " 'death',\n",
              " 'd',\n",
              " 'army',\n",
              " 'should',\n",
              " 'take',\n",
              " 'its',\n",
              " 'work',\n",
              " 'black',\n",
              " 'another',\n",
              " 'really',\n",
              " 'please',\n",
              " 'lol',\n",
              " 'wildfire',\n",
              " 'never',\n",
              " 'hot',\n",
              " 'god',\n",
              " 'read',\n",
              " 'mh370',\n",
              " 'look',\n",
              " 'help',\n",
              " 'bomber',\n",
              " 'fatal',\n",
              " 'am',\n",
              " 'any',\n",
              " 'live',\n",
              " 'where',\n",
              " 'northern',\n",
              " 'those',\n",
              " 'obama',\n",
              " '8',\n",
              " 'pm',\n",
              " 'japan',\n",
              " '11',\n",
              " 'school',\n",
              " 'much',\n",
              " 'feel',\n",
              " 'city',\n",
              " 'wild',\n",
              " 'flood',\n",
              " 'reddit',\n",
              " 'great',\n",
              " '9',\n",
              " 'stop',\n",
              " 'near',\n",
              " 'night',\n",
              " 'shit',\n",
              " 'said',\n",
              " 'latest',\n",
              " 'injured',\n",
              " 'before',\n",
              " '6',\n",
              " 'typhoon',\n",
              " 'top',\n",
              " 'homes',\n",
              " 'services',\n",
              " 'floods',\n",
              " 'under',\n",
              " 'hope',\n",
              " 'fear',\n",
              " 'ever',\n",
              " 'during',\n",
              " 'come',\n",
              " 'legionnaires',\n",
              " 'atomic',\n",
              " 'while',\n",
              " 'im',\n",
              " 'house',\n",
              " 'flames',\n",
              " '10',\n",
              " 'truck',\n",
              " 'state',\n",
              " 'post',\n",
              " 'getting',\n",
              " '15',\n",
              " 'wreck',\n",
              " 'ass',\n",
              " 'everyone',\n",
              " 'every',\n",
              " 'damage',\n",
              " 'content',\n",
              " 'change',\n",
              " 'red',\n",
              " 'earthquake',\n",
              " 'cross',\n",
              " 'summer',\n",
              " 'since',\n",
              " 'severe',\n",
              " 'plan',\n",
              " 'oil',\n",
              " 'coming',\n",
              " 'again',\n",
              " '7',\n",
              " 'weather',\n",
              " 'these',\n",
              " 'military',\n",
              " 'found',\n",
              " 'food',\n",
              " 'ûò',\n",
              " 'thunderstorm',\n",
              " 'heat',\n",
              " 'family',\n",
              " 'debris',\n",
              " 'without',\n",
              " 'other',\n",
              " 'next',\n",
              " 'little',\n",
              " 'wounded',\n",
              " 'through',\n",
              " 'natural',\n",
              " 'most',\n",
              " 'lightning',\n",
              " 'hit',\n",
              " 'gonna',\n",
              " 'evacuation',\n",
              " 'devastated',\n",
              " 'cause',\n",
              " 'always',\n",
              " 'also',\n",
              " 'well',\n",
              " 'smoke',\n",
              " 'set',\n",
              " 'does',\n",
              " 'times',\n",
              " 'survive',\n",
              " 'national',\n",
              " 'looks',\n",
              " 'free',\n",
              " 'destroyed',\n",
              " 'boy',\n",
              " 'terrorist',\n",
              " 'story',\n",
              " 'photo',\n",
              " 'murder',\n",
              " 'malaysia',\n",
              " 'injuries',\n",
              " 'face',\n",
              " 'check',\n",
              " 'bloody',\n",
              " 'bad',\n",
              " 'which',\n",
              " 'trapped',\n",
              " 'spill',\n",
              " 'someone',\n",
              " 'service',\n",
              " 'says',\n",
              " 'run',\n",
              " 'refugees',\n",
              " 'liked',\n",
              " 'fucking',\n",
              " 'flooding',\n",
              " 'failure',\n",
              " '70',\n",
              " 'women',\n",
              " 'until',\n",
              " 'tonight',\n",
              " 'show',\n",
              " 'oh',\n",
              " 'loud',\n",
              " 'fall',\n",
              " 'china',\n",
              " 'area',\n",
              " '30',\n",
              " '08',\n",
              " 'ûª',\n",
              " 'thunder',\n",
              " 'saudi',\n",
              " 'rain',\n",
              " 'o',\n",
              " 'movie',\n",
              " 'landslide',\n",
              " 'game',\n",
              " 'boat',\n",
              " 'around',\n",
              " 'week',\n",
              " 'weapon',\n",
              " 'terrorism',\n",
              " 'rescue',\n",
              " 'p',\n",
              " 'girl',\n",
              " 'confirmed',\n",
              " 'call',\n",
              " 'blood',\n",
              " 'bag',\n",
              " 'air',\n",
              " 'wind',\n",
              " 'survived',\n",
              " 'put',\n",
              " 'migrants',\n",
              " 'injury',\n",
              " 'head',\n",
              " 'hail',\n",
              " 'drought',\n",
              " 'weapons',\n",
              " 'survivors',\n",
              " 'screaming',\n",
              " 'report',\n",
              " 'released',\n",
              " 'panic',\n",
              " 'kills',\n",
              " 'hurricane',\n",
              " 'hostage',\n",
              " 'explosion',\n",
              " 'destroy',\n",
              " 'burned',\n",
              " 'big',\n",
              " 'whole',\n",
              " 'warning',\n",
              " 'save',\n",
              " 'rescued',\n",
              " 'missing',\n",
              " 'hazard',\n",
              " 'breaking',\n",
              " 'attacked',\n",
              " 'thing',\n",
              " 'past',\n",
              " 'mosque',\n",
              " 'made',\n",
              " 'heard',\n",
              " 'fuck',\n",
              " 'destruction',\n",
              " 'bridge',\n",
              " 'apocalypse',\n",
              " 'against',\n",
              " '40',\n",
              " 'violent',\n",
              " 'trauma',\n",
              " 'sinking',\n",
              " 'sinkhole',\n",
              " 'responders',\n",
              " 'rescuers',\n",
              " 'ok',\n",
              " 'hundreds',\n",
              " 'high',\n",
              " 'end',\n",
              " 'drowning',\n",
              " 'bags',\n",
              " 'wave',\n",
              " 'self',\n",
              " 'saw',\n",
              " 'real',\n",
              " 'massacre',\n",
              " 'keep',\n",
              " 'island',\n",
              " 'dust',\n",
              " 'demolished',\n",
              " 'deaths',\n",
              " 'crush',\n",
              " 'crashed',\n",
              " 'collapsed',\n",
              " 'catastrophic',\n",
              " '0',\n",
              " 'white',\n",
              " 'use',\n",
              " 'stock',\n",
              " 'river',\n",
              " 'lives',\n",
              " 'lava',\n",
              " 'hazardous',\n",
              " 'explode',\n",
              " 'evacuate',\n",
              " 'due',\n",
              " 'county',\n",
              " 'blown',\n",
              " 'wreckage',\n",
              " 'very',\n",
              " 'update',\n",
              " 'trouble',\n",
              " 'traumatised',\n",
              " 'tragedy',\n",
              " 'structural',\n",
              " 'screamed',\n",
              " 'quarantine',\n",
              " 'outbreak',\n",
              " 'must',\n",
              " 'market',\n",
              " 'long',\n",
              " 'light',\n",
              " 'exploded',\n",
              " 'drown',\n",
              " 'displaced',\n",
              " 'derailment',\n",
              " 'collision',\n",
              " 'collided',\n",
              " 'charged',\n",
              " 'catastrophe',\n",
              " 'bang',\n",
              " 'away',\n",
              " 'august',\n",
              " 'ambulance',\n",
              " 'screams',\n",
              " 'ruin',\n",
              " 'quarantined',\n",
              " 'meltdown',\n",
              " 'least',\n",
              " 'group',\n",
              " 'electrocuted',\n",
              " 'crushed',\n",
              " 'calgary',\n",
              " 'bombed',\n",
              " 'blew',\n",
              " 'bleeding',\n",
              " 'battle',\n",
              " '05',\n",
              " '00',\n",
              " 'wounds',\n",
              " 'three',\n",
              " 'suspect',\n",
              " 'riot',\n",
              " 'part',\n",
              " 'panicking',\n",
              " 'obliteration',\n",
              " 'mudslide',\n",
              " 'investigators',\n",
              " 'inundated',\n",
              " 'harm',\n",
              " 'flattened',\n",
              " 'engulfed',\n",
              " 'deluged',\n",
              " 'curfew',\n",
              " 'caused',\n",
              " 'blast',\n",
              " 'better',\n",
              " 'bagging',\n",
              " 'b',\n",
              " 'arson',\n",
              " 'anniversary',\n",
              " 'airplane',\n",
              " 'wrecked',\n",
              " 'windstorm',\n",
              " 'twister',\n",
              " 'thought',\n",
              " 'something',\n",
              " 'sirens',\n",
              " 'seismic',\n",
              " 'sandstorm',\n",
              " 'rioting',\n",
              " 'obliterated']"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tT3llt2Yzvi"
      },
      "source": [
        "### Experiment! Change `max_feature` to `100`. Check out what do you get. \n",
        "Remember to change it back to 500 or more afterwards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO3_p7s4Yzvl"
      },
      "source": [
        "### Finally we build our bag of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0b7fri1Yzvq"
      },
      "source": [
        "# define a function bagofwords with the following input: sentence and words\n",
        "def bagofwords(sentence, words):\n",
        "    sentence_words = extract_words(sentence) #tokenize sentences/ tweets and assign it to variable sentence_words\n",
        "    # frequency word count\n",
        "    bag = np.zeros(len(words)) #create a NumPy array made up of zeroes with size len(words)\n",
        "    # loop through data and add value of 1 when token is present in the tweet\n",
        "    for sw in sentence_words:\n",
        "        for i,word in enumerate(words):\n",
        "            if word == sw: \n",
        "                bag[i] += 1\n",
        "                \n",
        "    return np.array(bag) # return the bag of word for one tweet"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP0K3VipYzvu"
      },
      "source": [
        "### Task: Test your function using a made up text data.\n",
        "Look at the list of words above to see what words you might want to add into your sample text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "N5eOi87hYzvv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "656da200-f967-4e51-8d8f-c50f73bcf8b2"
      },
      "source": [
        "text = 't co http  in for'\n",
        "bagofwords(text, vocab)"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7OJYBtrYzv1"
      },
      "source": [
        "Notice your one row of bag of words! \n",
        "\n",
        "Now, we want to loop this function through our whole dataset. See below for how we do this. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c_o3jjbYzv3"
      },
      "source": [
        "# set up a NumPy array with the specified dimension to contain the bag of words\n",
        "n_words = len(vocab)\n",
        "n_docs = len(df)\n",
        "bag_o = np.zeros([n_docs,n_words])\n",
        "# use loop function to add new row for each tweet. \n",
        "for ii in range(n_docs): \n",
        "    #call out the previous function 'bagofwords'. see the inputs: sentence and words\n",
        "    bag_o[ii,:] = bagofwords(df['text'].iloc[ii], vocab) "
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsQtZ8XmYzv9"
      },
      "source": [
        "Now, find out the [dimension](https://stackoverflow.com/questions/14847457/how-do-i-find-the-length-or-dimensions-size-of-a-numpy-matrix-in-python) of the numpy array. Does it make sense to you?\n",
        "\n",
        "<font color=red>Hint:</font> Check out the given link: https://stackoverflow.com/questions/14847457/how-do-i-find-the-length-or-dimensions-size-of-a-numpy-matrix-in-python \n",
        "\n",
        "<font color=red>Your code:</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdK_lgpeYzv-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf5d9ece-af4c-4095-d4d6-5ea18f968347"
      },
      "source": [
        "bag_o.shape"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10860, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjQiUGvTYzwD"
      },
      "source": [
        "## 2. Find out the total frequency, inverse document frequency\n",
        "\n",
        "Here, we would like to work with words that provide us with the most meaning in the sentences/ tweets. Does it make sense to think that the words that are used most often are important?\n",
        "\n",
        "We first take a look at the words inside our bag of words. Print the 20 most frequent words.  \n",
        "hint: Your hash_map is a dictionary of all words with each word as a key, and its frequency as a value aka dict{word: frequency}. What do you notice about these most common words?\n",
        "\n",
        "See [this article](https://docs.python.org/3/howto/sorting.html) under 'Key Functions' to find out. Use object’s indices as keys.\n",
        "\n",
        "<font color=red>Hints:</font> \n",
        "*   check out the given link: https://docs.python.org/3/howto/sorting.html \n",
        "*   look out for '`key=lambda`' and '`reverse`'\n",
        "\n",
        "<font color=red>Your code:</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HJlCylND0dO",
        "outputId": "ba2dec09-7617-43f9-96f8-2770c2e94074"
      },
      "source": [
        "from collections import Counter\n",
        "word_counts = Counter(hash_map)\n",
        "top_20 = word_counts.most_common(20)\n",
        "print(top_20)\n",
        "\n"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('t', 7447), ('co', 6800), ('http', 6154), ('in', 2807), ('i', 2507), ('s', 1276), ('for', 1243), ('on', 1236), ('that', 852), ('with', 797), ('by', 777), ('at', 748), ('this', 702), ('https', 618), ('from', 613), ('be', 596), ('was', 553), ('û_', 514), ('have', 513), ('amp', 510)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWRbT6N5YzxG"
      },
      "source": [
        "What is `key=lambda` for? See [this article](https://stackoverflow.com/questions/13669252/what-is-key-lambda/13669294) to find out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oucWUtONYzxI"
      },
      "source": [
        "See the top 20 words above. What do you notice?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqFDbt80YzxK"
      },
      "source": [
        "### Choosing features\n",
        "The 20 most common words give almost NO information about the tweets. They are remains of twitter urls, as well as some common words frequently found in all text. We can hardly consider them 'important features'. It seems then that to improve the model, we should do more than just look at the most frequent words. \n",
        "\n",
        "Perhaps we should look for words that appear frequently in some documents, but not in all documents. Why do you think this makes sense?\n",
        "\n",
        "This is the intuition behind what is known as 'total frequency-inverse document frequency', or tfidf.  \n",
        "\n",
        "\n",
        "The tfidf formula is below: \n",
        "$$w_{i,j}=tf_{i,j}*log(\\frac{N}{df_i})$$  \n",
        "In this formula, $i$ is a word indexer and $j$ is a document indexer.  \n",
        "In your bag of words, each row is a document, while each column is the frequency of a word in that document. This is already the 'term frequency' portion of tfidf ($tf_{i,j}$). \n",
        "\n",
        "### Inverse document frequency\n",
        "\n",
        "We now want to calculate the inverse document frequency, which can be understood in the following way: for each word, count the number of documents it appears in, and then take the log of the inverse of that number.  \n",
        "\n",
        "Build the idf vector in 2 parts. \n",
        "1. First, build the word frequency for each word. \n",
        "2. Then divide the number of documents (N) by the word frequency and take the log of the result.\n",
        "\n",
        "Remember what we have done during the acquire phase?\n",
        "\n",
        "<font color=red>Hints:</font>\n",
        "*   Please do not miss out '`#your code here`'\n",
        "*   `numwords`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NxWkhFOkNN4",
        "outputId": "8d5f7dc6-3c49-442d-a74a-6a763f437b53"
      },
      "source": [
        "print(bag_o.shape)"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10860, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHHKY2ZEYzxL"
      },
      "source": [
        "#initialize 2 variables representing the number of tweets (numdocs) and the number of tokens/words (numwords)\n",
        "numdocs, numwords = np.shape(bag_o)\n",
        "#10860  # 500 \n",
        "#numdocs = 10860\n",
        "#numwords = 500\n",
        "\n",
        "#Changing into the tfidf formula as above\n",
        "N = numdocs\n",
        "word_frequency = np.empty(numwords)\n",
        "\n",
        "#Count the number of documents the word appears in.\n",
        "for word in range(numwords):\n",
        "    word_frequency[word]=np.sum((bag_o[:,word]>0)) \n",
        "\n",
        "idf = np.log(N/word_frequency) # number of docs / number of docs the word appears in"
      ],
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w30a1Agpf9s",
        "outputId": "3aa2386d-4b83-4903-b41c-c1ab196d3deb"
      },
      "source": [
        "idf.shape"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500,)"
            ]
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz9VuXgLYzxu"
      },
      "source": [
        "Now we will complete our `tfidf` by multiplying our bag of words (term frequency) with the idf\n",
        "\n",
        "<font color=red>Hints:</font>\n",
        "*   Please do not miss out '`#your code here`'\n",
        "*   `numdocs`, `numwords`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuGid6G3Yzxv"
      },
      "source": [
        "#initializs tfidf array\n",
        "tfidf = np.empty([numdocs, numwords])\n",
        "\n",
        "#loop through the tweets, multiply term frequency (represented by bag of words) with idf\n",
        "for doc in range(numdocs):\n",
        "    tfidf[doc, :]=bag_o[doc, :]*idf"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "mVqSJuGFYzx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1f2849b-61dd-4fe8-9a60-a1b040760a28"
      },
      "source": [
        "tfidf.shape"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10860, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W96G2dKBYzx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f289f1d-51e9-4137-ea8f-8cafcd80917b"
      },
      "source": [
        "print (tfidf)"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.56037575 0.64339282 0.74842242 ... 0.         0.         0.        ]\n",
            " [0.56037575 0.64339282 0.74842242 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQE77X3DYzyC"
      },
      "source": [
        "How would you describe the `tfidf` array? It is made of the tfidf value of each of the 500 token in the 10860 tweets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6octxAt9YzyC"
      },
      "source": [
        "## 3. Train your model with Machine Learning\n",
        "Now that you finally have your `tfidf` array. It is time to train your model and to make predictions! We will be using the scikit learn library, which provides a number of machine learning models. \n",
        "\n",
        "Do you remember what is machine learning? It is an application of AI that allows a system to automatically learn without being explicitly programmed. \n",
        "\n",
        "Now, we are using what we call the supervised learning. Do you remember what this is?\n",
        "This is the type of learning that enable us to make models to predict certain system, given a given training set. In this case, we want to predict if a text relates to news about disaster or not. We have already downloaded text data from twitter, and we have labelled the data with several labels i.e. 'Relevant', 'Not Relevant', and 'Can't decide'. These data will be used to train our model. Let's find out how we can do this!\n",
        "\n",
        "Let's first download the libraries required to do this. The scikit learn library contains numerous useful functions to be used for machine learning problem. \n",
        "\n",
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4gdjsOzYzyD"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression #to import logistic regression model\n",
        "from sklearn.model_selection import train_test_split #to split data into training and testing set\n",
        "from sklearn.model_selection import GridSearchCV #to find out the best parameter for our model"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx-TTIzEYzyE"
      },
      "source": [
        "### Step 1. Split data into training and test set\n",
        "\n",
        "Before training our model, we will split our dataset into 2: a training set, and a test set.\n",
        "\n",
        "We will then train our model on the training set, and then test the model generated during the training stage on the test set. This is to ensure that the testing is done on dataset that the model has never 'seen' processed. \n",
        "\n",
        "A good starting point for the split is to have 80% of your data in the train set, and 20% of the data in the test set.  \n",
        "\n",
        "Let's do this now!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPEIcdyTYzyH"
      },
      "source": [
        "# split X_all and y_all into training and testing sets\n",
        "X_train,X_test,y_train,y_test = train_test_split(tfidf,df['relevance'].values,shuffle=True)"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2fv4pYiYzyJ"
      },
      "source": [
        "Could you explain what is happening on the code above?. We are using the train_test_split function to split the tfidf array and part of the initial dataframe which include the 'relevance' value of our tweet. What does [shuffle=True](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) means?\n",
        "\n",
        "Let's learn more about the dataset we are working with! print tfidf and df['relevance'] below to see them. Meanwhile, find out what is [.value](https://www.geeksforgeeks.org/python-pandas-dataframe-values/) for. \n",
        "\n",
        "<font color=red>Your code:</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIS-DUJyYzyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51aea86b-3d4a-4590-9bf2-db276d064a74"
      },
      "source": [
        "print(tfidf) \n",
        "print(df['relevance'])\n",
        "# Relevant = 1\n",
        "# Not Relevant = 0"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.56037575 0.64339282 0.74842242 ... 0.         0.         0.        ]\n",
            " [0.56037575 0.64339282 0.74842242 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
            "0        1\n",
            "1        1\n",
            "2        1\n",
            "3        1\n",
            "4        1\n",
            "        ..\n",
            "10871    1\n",
            "10872    1\n",
            "10873    1\n",
            "10874    1\n",
            "10875    1\n",
            "Name: relevance, Length: 10860, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwZZCKcpYzyM"
      },
      "source": [
        "Great! Now that we have already split our dataset, let's see what data we have now. Print the shape of `X_train`, `X_test`, `y_train` and `y_test`! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d63XOG0WYzyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb2012e4-3a9b-4aee-cabe-ad832a0fabc6"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8145, 500)\n",
            "(2715, 500)\n",
            "(8145,)\n",
            "(2715,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRQMOEatYzyS"
      },
      "source": [
        "Compare *X_train* and `X_test`, and also compare `y_train` and `y_test`. What do you notice about the difference in shape? \n",
        "\n",
        "<font color=red>Your answer:</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkxozwFGYzyS"
      },
      "source": [
        "# for X_train and X_test:\n",
        "# X_train -> larger training shape\n",
        "# X_test -> smaller test size shape\n",
        "\n",
        "# for y_train and y_test:\n",
        "# y_train -> larger training shape  -> no numwords shape\n",
        "# y_test -> smaller test size shape -> no numwords shape"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmCGmVw_YzyV"
      },
      "source": [
        "### Step 2. Create a model instance\n",
        "After splitting the data, we will make an instance of the model i.e. we are simply initialising the model. In this task, we are using the logistic regressor, which is useful when we want to categorise data. Read [here](https://towardsdatascience.com/understanding-logistic-regression-9b02c2aec102) to find out more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIwjlpJZYzyW"
      },
      "source": [
        "# Create a model instance\n",
        "logreg = LogisticRegression(solver = 'lbfgs')"
      ],
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgrtWRPhYzyX"
      },
      "source": [
        "### Step 3. Train the model on data, store the information learnt from data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "RZQa4xoRYzyY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de47bc6a-4681-48bc-a741-ebdd1c97450b"
      },
      "source": [
        "#Fit the model on the training set\n",
        "logreg.fit(X_train,y_train)"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huTjy4ZbYzyc"
      },
      "source": [
        "Do not worry about understanding the parameters shown above for now. You can still create projects with the logistic regressor even if you do not know every parameter involved. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXHf-6qjYzyd"
      },
      "source": [
        "## Step 4. Use model to predict relevance based on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMJ8Y0ufYzyd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5566ed27-bf71-4f29-9df5-ae9468b325ef"
      },
      "source": [
        "y_pred=logreg.predict(X_test)\n",
        "print (y_pred)"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1 ... 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bju_U2mcYzyg"
      },
      "source": [
        "What does the `y_pred` mean?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exf8znmFYzyi"
      },
      "source": [
        "We will now measure our model performance by finding out the accuracy value of the model. Remember, accuracy is defined as:\n",
        "\n",
        "Fraction of correct predictions = correct predictions / total number of data points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm5_KXLYYzyi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b9390e1-9d15-424e-a077-51f13015a8f4"
      },
      "source": [
        "# Use score method to get accuracy of model\n",
        "score = logreg.score(X_test, y_test)\n",
        "print(score)\n",
        "\n",
        "print('Accuracy of logistic regression classifier on test set: {:.3f}'.format(score))"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7775322283609576\n",
            "Accuracy of logistic regression classifier on test set: 0.778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP9Z4EazYzyl"
      },
      "source": [
        "Awesome! we have collected our data, process them, split them into training and testing data, train our model, and evaluate performance of our model. Next, we will define a function that will help us do all these task in one go! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkewC5YWYzym"
      },
      "source": [
        "### Model Training Pipeline\n",
        "\n",
        "Your task is to build a function that will:\n",
        "1. Take in the untrained model, tfidf array, and the values from the training target.\n",
        "2. Randomly split the two into a train and test set\n",
        "3. Fit the model on the training set\n",
        "4. Print the accuracy score on the test set\n",
        "5. Return the trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y32nI43Yzym"
      },
      "source": [
        "def classify(rf, X_all, y_all): #Take in the untrained model, tfidf array, and the values from the training target.\n",
        "    X_train,X_test,y_train,y_test = train_test_split(X_all,y_all,shuffle=True) #Randomly split the two into a train and test set\n",
        "    logreg.fit(X_train,y_train) #Fit the model on the training set\n",
        "    print(rf.score(X_test,y_test)) #Print the score on the test set\n",
        "    return logreg #Return the trained model."
      ],
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b8PxUJrYzyp"
      },
      "source": [
        "Now we can apply the function into our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoXXaI0SYzyu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73d85cae-d036-4dd5-c089-ec69b96f8f16"
      },
      "source": [
        "logreg = LogisticRegression(solver = 'lbfgs')\n",
        "#logreg = LogisticRegression()\n",
        "\n",
        "X_all = tfidf\n",
        "y_all = df['relevance'].values\n",
        "logreg = classify(logreg, X_all, y_all)"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7804788213627992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv-YHPYJYzyx"
      },
      "source": [
        "### Tuning parameters (Optional)\n",
        "If you have implemented this right, you should have a test score of at least 0.75. \n",
        "\n",
        "Let us try to improve this score by tuning the hyperparameters. We first take a look at the parameters in your logistic regressor. Feel free to read more about its parameters [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Again, don't stress yourself out if you do not understand these parameters now. It will come with more reading and practise!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VaT5lXEYzyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae2377bd-5c5e-4a81-d331-b7c0c0ced28b"
      },
      "source": [
        "logreg"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCpx_qMXYzy1"
      },
      "source": [
        "Parameters can be automatically tuned using scikit-learn's GridSearchCV. This function takes in a model, as well as a dictionary of parameters with values to test, and runs tests each combination of parameters to find the optimal combination for the highest score. Learn more [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIy6HbTHYzy1"
      },
      "source": [
        "# Define your hyperparameters here\n",
        "parameters = {'C':[0.001, 0.01, 0.1, 1, 10], 'tol':[0.0001, 0.001, 0.01], 'max_iter':[100, 1000]}"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwMXMDzZYzy4"
      },
      "source": [
        "clf = GridSearchCV(logreg, parameters, cv=3, return_train_score=True)"
      ],
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "orzHGZY4Yzy7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b4d93aa-9bfe-4179-e756-3d581ce6a42c"
      },
      "source": [
        "clf.fit(X_all, y_all)"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=3, error_score=nan,\n",
              "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                          fit_intercept=True,\n",
              "                                          intercept_scaling=1, l1_ratio=None,\n",
              "                                          max_iter=100, multi_class='auto',\n",
              "                                          n_jobs=None, penalty='l2',\n",
              "                                          random_state=None, solver='lbfgs',\n",
              "                                          tol=0.0001, verbose=0,\n",
              "                                          warm_start=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10],\n",
              "                         'max_iter': [100, 1000],\n",
              "                         'tol': [0.0001, 0.001, 0.01]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYceJ02zYzzD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "461ab08c-7faf-4d5c-ea36-849abb9fc88f"
      },
      "source": [
        "# You can view the raw results using clf.cv_results_\n",
        "print(clf.best_params_, clf.best_score_)"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'C': 0.01, 'max_iter': 100, 'tol': 0.0001} 0.7246777163904236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GakroATKYzzG"
      },
      "source": [
        "See the accuracy result. Do you notice that it drops? Why do think this is so?\n",
        "\n",
        "The accuracy might be different because we split our models randomly. What we can do is to repeat the model fitting multiple times and get the average accuracy result. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weAr26dFYzzH"
      },
      "source": [
        "## 4. Build your pipeline\n",
        "At this point you have a trained and optimized model ready to predict tweets. You will now tie it all together to build a pipeline for prediction. \n",
        "\n",
        "This will be a function that:\n",
        "1. Take in a tweet in the form of a string\n",
        "2. Prints a prediction on whether the tweet is relevant\n",
        "\n",
        "<font color=red>Your code:</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iiv2UFs0YzzH"
      },
      "source": [
        "def twitter_predictor(tweet):\n",
        "    word_vector = bagofwords(tweet, vocab) # set a variable with bag of words. Remember the bagofwords function you have created?\n",
        "    word_tfidf =  word_vector * idf #find tfidf value => Hint: word_vector, idf\n",
        "    prediction = logreg.predict(word_tfidf.reshape(1, -1)) #predict wether a tweet is relevant or not relevant to natural disaster\n",
        "    results = {1:'Relevant', 0:'Not Relevant'} #creating a set containing the potential results. You can change the 'Relevant' and 'Not relevant' tag\n",
        "    print(results[int(prediction)])"
      ],
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSqEwQ6rYzzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3d70d57-6257-4b85-c4d9-62d9607e710a"
      },
      "source": [
        "tweet1 = 'When the aftershock happened (Nepal) we were the last intl team still there; in a way we were 1st responders'\n",
        "tweet2 = 'NLP is fun and I learnt so much today.'\n",
        "twitter_predictor(tweet1)\n",
        "twitter_predictor(tweet2)"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevant\n",
            "Not Relevant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ponydJGkYzzK"
      },
      "source": [
        "### Task: Write your own tweets and see if your model can classify them!\n",
        "\n",
        "<font color=red>Your code:</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "0ycK5b6h7E6J",
        "outputId": "a97e0ad2-e48a-4ebb-a9ce-2e0b66d13b82"
      },
      "source": [
        "df"
      ],
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>choose_one</th>\n",
              "      <th>relevance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10871</th>\n",
              "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10872</th>\n",
              "      <td>Police investigating after an e-bike collided ...</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10873</th>\n",
              "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10874</th>\n",
              "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10875</th>\n",
              "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
              "      <td>Relevant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10860 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text choose_one  relevance\n",
              "0                     Just happened a terrible car crash   Relevant          1\n",
              "1      Our Deeds are the Reason of this #earthquake M...   Relevant          1\n",
              "2      Heard about #earthquake is different cities, s...   Relevant          1\n",
              "3      there is a forest fire at spot pond, geese are...   Relevant          1\n",
              "4                 Forest fire near La Ronge Sask. Canada   Relevant          1\n",
              "...                                                  ...        ...        ...\n",
              "10871  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   Relevant          1\n",
              "10872  Police investigating after an e-bike collided ...   Relevant          1\n",
              "10873  The Latest: More Homes Razed by Northern Calif...   Relevant          1\n",
              "10874  MEG issues Hazardous Weather Outlook (HWO) htt...   Relevant          1\n",
              "10875  #CityofCalgary has activated its Municipal Eme...   Relevant          1\n",
              "\n",
              "[10860 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 255
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "iZkX2xtpYzzL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b517c6ed-2ee3-414d-dd3d-2e25fc75d315"
      },
      "source": [
        "my_tweet_1 = 'Just happened a terrible car crash'\n",
        "my_tweet_2 = 'Forest fire near La Ronge Sask. Canada'\n",
        "\n",
        "twitter_predictor(my_tweet_1)\n",
        "twitter_predictor(my_tweet_2)"
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not Relevant\n",
            "Relevant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfBuuX7XYzzN"
      },
      "source": [
        "Is your model able to give you the right result? \n",
        "\n",
        "What do you think we can do to improve the model performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMkMM-4DYzzO"
      },
      "source": [
        "Congratulations! You have now built your very own machine learning NLP model.  \n",
        "  \n",
        "# 5. NLP classification challenge!\n",
        "Now that you have learnt the basics of using the bag-of-words normalized by TFIDF to do classification of natural language data, it is time put your skills to the test!  \n",
        "\n",
        "## Sentiment analysis\n",
        "An important application of the classification of natural text is in _sentiment analysis_. Sentiment analysis is the process of categorizing opinions in a piece of text in order to identify the writer's attitude toward a particular topic.  \n",
        "  \n",
        "In this challenge, we will be classifying movie reviews from [imdb](https://www.imdb.com/). The data has already been stored as two .pkl files (for now, just understand these as file types that can be read using python), one for the training data, and one on the test data. \n",
        "\n",
        "You will have to process and train your model on the train dataset of movie reviews `df_raw.pkl`, and then report the accuracy of your model on the test move reviews `df_raw_test.pkl`.  \n",
        "  \n",
        "You will be predicting if a review has either a positive or negative sentiment. Positive sentiments are labeled as 1, while negative sentiments are labeled as 0.\n",
        "  \n",
        "In this segment we utilize a few new functions provided by sklearn. \n",
        "1. You can generate your bag of words and condition it using TFIDF with the functions you have built earlier. \n",
        "2. Alternatively, the [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) function can be used to create your bag of words. Use the `max_features=5000` argument to only select the top 5000 most common words. \n",
        "3. You can also use the [`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) to help transform your bag of words with TFIDF.  \n",
        "  \n",
        "The dataframes for the train and test dataset has already been imported for you. You will have to make use of the skills you have learnt earlier and in Experience 1 to preprocess, vectorize (with the bag of words), transform (with TFIDF), fit, and predict the movie review sentiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrEUNlMIYzzP"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer # This function helps you create your bag of words\n",
        "from sklearn.feature_extraction.text import TfidfTransformer # This function automatically normalizes your bag of words\n",
        "df_raw = pd.read_pickle('/content/Intel_AI4Y/My Drive/Intel_AI4Y/Students_E_Learning/Copy_To_Google_Drive/Intel_AI4Y_Colab/Module_23/imdb/df_raw.pkl')\n",
        "df_raw_test = pd.read_pickle('/content/Intel_AI4Y/My Drive/Intel_AI4Y/Students_E_Learning/Copy_To_Google_Drive/Intel_AI4Y_Colab/Module_23/imdb/df_raw_test.pkl')"
      ],
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj0BPeEJYzzT"
      },
      "source": [
        "To start off, try printing a sample of the test dataset. What are the names of the columns?\n",
        "\n",
        "<font color=red>Your code:</font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3feGqtSYzzU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "outputId": "64e25888-9578-44be-c4cc-21e6b1be31d6"
      },
      "source": [
        "df_raw_test.head(30)"
      ],
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>scores</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I went and saw this movie last night after bei...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>As a recreational golfer with some knowledge o...</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>I saw this film on September 1st, 2005 in Indi...</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Maybe I'm reading into this too much, but I wo...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I felt this film did have many good qualities....</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>This movie is amazing because the fact that th...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>\"Quitting\" may be as much about exiting a pre-...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>I loved this movie from beginning to end.I am ...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>I was fortunate to attend the London premier o...</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>I first saw this movie on IFC. Which is a grea...</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>I must say, every time I see this movie, I am ...</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>My wife is a mental health therapist and we wa...</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>I saw this film at the Rotterdam International...</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>\"Night of the Hunted\" stars French porn star B...</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Even if you're a fan of Jean Rollin's idiosync...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>I was surprised how much I enjoyed this. Sure ...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>I went into \"Night of the Hunted\" not knowing ...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>I have certainly not seen all of Jean Rollin's...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Since this cartoon was made in the old days, F...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Despite the title and unlike some other storie...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Felix in Hollywood is a great film. The versio...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>A gem of a cartoon from the silent era---it wa...</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>This short is one of the best of all time and ...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Felix is watching an actor rehearse his lines:...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>While I can't say whether or not Larry Hama ev...</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Errol Flynn's roguish charm really shines thro...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Warner Brothers tampered considerably with Ame...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 text  scores  positive\n",
              "0   I went and saw this movie last night after bei...      10         1\n",
              "1   Actor turned director Bill Paxton follows up h...       7         1\n",
              "2   As a recreational golfer with some knowledge o...       9         1\n",
              "3   I saw this film in a sneak preview, and it is ...       8         1\n",
              "4   Bill Paxton has taken the true story of the 19...       8         1\n",
              "5   I saw this film on September 1st, 2005 in Indi...       9         1\n",
              "6   Maybe I'm reading into this too much, but I wo...       8         1\n",
              "7   I felt this film did have many good qualities....       7         1\n",
              "8   This movie is amazing because the fact that th...      10         1\n",
              "9   \"Quitting\" may be as much about exiting a pre-...       8         1\n",
              "10  I loved this movie from beginning to end.I am ...      10         1\n",
              "11  I was fortunate to attend the London premier o...       9         1\n",
              "12  I first saw this movie on IFC. Which is a grea...       9         1\n",
              "13  I must say, every time I see this movie, I am ...       9         1\n",
              "14  My wife is a mental health therapist and we wa...       9         1\n",
              "15  I saw this film at the Rotterdam International...       9         1\n",
              "16  \"Night of the Hunted\" stars French porn star B...       7         1\n",
              "17  Even if you're a fan of Jean Rollin's idiosync...       8         1\n",
              "18  I was surprised how much I enjoyed this. Sure ...       8         1\n",
              "19  I went into \"Night of the Hunted\" not knowing ...       8         1\n",
              "20  I have certainly not seen all of Jean Rollin's...       8         1\n",
              "21  Since this cartoon was made in the old days, F...       8         1\n",
              "22  Despite the title and unlike some other storie...      10         1\n",
              "23  Felix in Hollywood is a great film. The versio...       8         1\n",
              "24  A gem of a cartoon from the silent era---it wa...       9         1\n",
              "25  This short is one of the best of all time and ...      10         1\n",
              "26  Felix is watching an actor rehearse his lines:...       8         1\n",
              "27  While I can't say whether or not Larry Hama ev...       9         1\n",
              "28  Errol Flynn's roguish charm really shines thro...       8         1\n",
              "29  Warner Brothers tampered considerably with Ame...      10         1"
            ]
          },
          "metadata": {},
          "execution_count": 262
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVVUOco8YzzW"
      },
      "source": [
        "Now, process your text using the `CountVectorizer` to create your bag of words. you can create a class of `CountVectorizer()`, and use the method `.fit_transform()` with the text as argument to build your bag of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKc9I-aLYzzW"
      },
      "source": [
        "vectorizer = CountVectorizer(analyzer = \"word\", strip_accents=None, tokenizer = None, \\\n",
        "                             preprocessor = None, stop_words = None, max_features = 5000) \n",
        "train_data_features = vectorizer.fit_transform(df_raw['text']) # docs\n",
        "test_data_features = vectorizer.transform(df_raw_test['text']) # docs"
      ],
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHKE0TzOYzzY"
      },
      "source": [
        "Now normalize your bag of words using the `TfidfTransformer`. Use it the same way as above. Create a class, and use the `.fit_transform()` method with the bag of words as your argument to create your TFIDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhqUdmp5YzzY"
      },
      "source": [
        "tfidfier = TfidfTransformer()\n",
        "tfidf = tfidfier.fit_transform(train_data_features) # df_raw\n",
        "tfidf_test = tfidfier.transform(test_data_features) # df_raw_test"
      ],
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNHiav2DYzzb"
      },
      "source": [
        "Now, use your transformed bag of words as the features to train and test your model like we did before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQp6IAjhYzzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daa3be05-f6c5-47a9-9d19-5e40a4c37f1c"
      },
      "source": [
        "X_all = tfidf.toarray()\n",
        "y_all = df_raw['positive'].values\n",
        "X_test = tfidf_test.toarray()\n",
        "y_test = df_raw_test['positive'].values\n",
        "def classify():\n",
        "    rf = LogisticRegression()\n",
        "    rf.fit(X_all,y_all)\n",
        "    print(rf.score(X_test,y_test))\n",
        "    return rf\n",
        "classify()"
      ],
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.88252\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 265
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p19UBH5hYzzg"
      },
      "source": [
        "You should have an accuracy of 80% without any hyperparameter tuning. Try and get the best accuracy on the test set that you can! This dataset is one of the hallmarks of natural language processing and is the entry-point for many aspiring data scientists and engineers. You can [see the original competition here](https://www.kaggle.com/c/word2vec-nlp-tutorial), and look at the different solutions other people have created!  \n",
        "\n",
        "## Next up, you can create a function to input your own review and get the model to predict if your sentence is positive or negative!\n",
        "\n",
        "# Congratulations!\n",
        "You have learnt how to build a natural language text classifier! "
      ]
    }
  ]
}